{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTVJtT9SdUaVqe0gOS+Dgl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KolossalKode/FastCap_CRM/blob/main/Strat%20Basic%20Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdCZQ17CiRSm",
        "outputId": "0e3dfbed-95a5-4f7f-e332-c0d9598dd5f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: alpha_vantage in /usr/local/lib/python3.11/dist-packages (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from alpha_vantage) (3.11.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from alpha_vantage) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
            "Requirement already satisfied: choreographer>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from kaleido) (1.0.9)\n",
            "Requirement already satisfied: logistro>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from kaleido) (1.1.0)\n",
            "Requirement already satisfied: orjson>=3.10.15 in /usr/local/lib/python3.11/dist-packages (from kaleido) (3.10.18)\n",
            "Requirement already satisfied: simplejson>=3.19.3 in /usr/local/lib/python3.11/dist-packages (from choreographer>=1.0.5->kaleido) (3.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (1.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->alpha_vantage) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->alpha_vantage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->alpha_vantage) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->alpha_vantage) (2025.6.15)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.1.2->aiohttp->alpha_vantage) (4.14.1)\n",
            "Requirement already satisfied: google-colab in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: google-auth==2.38.0 in /usr/local/lib/python3.11/dist-packages (from google-colab) (2.38.0)\n",
            "Requirement already satisfied: ipykernel==6.17.1 in /usr/local/lib/python3.11/dist-packages (from google-colab) (6.17.1)\n",
            "Requirement already satisfied: ipyparallel==8.8.0 in /usr/local/lib/python3.11/dist-packages (from google-colab) (8.8.0)\n",
            "Requirement already satisfied: ipython==7.34.0 in /usr/local/lib/python3.11/dist-packages (from google-colab) (7.34.0)\n",
            "Requirement already satisfied: notebook==6.5.7 in /usr/local/lib/python3.11/dist-packages (from google-colab) (6.5.7)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (from google-colab) (2.2.2)\n",
            "Requirement already satisfied: portpicker==1.5.2 in /usr/local/lib/python3.11/dist-packages (from google-colab) (1.5.2)\n",
            "Requirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.11/dist-packages (from google-colab) (2.32.3)\n",
            "Requirement already satisfied: tornado==6.4.2 in /usr/local/lib/python3.11/dist-packages (from google-colab) (6.4.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth==2.38.0->google-colab) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth==2.38.0->google-colab) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth==2.38.0->google-colab) (4.9.1)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (24.0.1)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (5.7.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipyparallel==8.8.0->google-colab) (4.4.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from ipyparallel==8.8.0->google-colab) (0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from ipyparallel==8.8.0->google-colab) (2.9.0.post0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from ipyparallel==8.8.0->google-colab) (4.67.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google-colab) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google-colab) (0.19.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google-colab) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google-colab) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google-colab) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google-colab) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google-colab) (4.9.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.7->google-colab) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.7->google-colab) (25.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.7->google-colab) (5.8.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.7->google-colab) (0.2.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.7->google-colab) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.7->google-colab) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.7->google-colab) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.7->google-colab) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.7->google-colab) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.7->google-colab) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->google-colab) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->google-colab) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->google-colab) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->google-colab) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->google-colab) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->google-colab) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->google-colab) (2025.6.15)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython==7.34.0->google-colab) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.1->notebook==6.5.7->google-colab) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook==6.5.7->google-colab) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.7->google-colab) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook==6.5.7->google-colab) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.7->google-colab) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.7->google-colab) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.7->google-colab) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.7->google-colab) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.7->google-colab) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.7->google-colab) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook==6.5.7->google-colab) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook==6.5.7->google-colab) (4.24.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython==7.34.0->google-colab) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.34.0->google-colab) (0.2.13)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth==2.38.0->google-colab) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->ipyparallel==8.8.0->google-colab) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook==6.5.7->google-colab) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook==6.5.7->google-colab) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook==6.5.7->google-colab) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.7->google-colab) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.7->google-colab) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.7->google-colab) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.7->google-colab) (0.26.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.7->google-colab) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook==6.5.7->google-colab) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook==6.5.7->google-colab) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook==6.5.7->google-colab) (4.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook==6.5.7->google-colab) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.7->google-colab) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.7->google-colab) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.7->google-colab) (1.3.1)\n",
            "OK: API Key from Colab 'API_Key'.\n",
            "OK: Alpha Vantage client initialized.\n",
            "--- Starting Sequential Analysis at 2025-07-11 12:52:18 ---\n",
            "\n",
            "--- Processing Symbol: SPY ---\n",
            " Phase 1: Fetching Data for SPY\n",
            "   Attempt 1/3: Fetching 15min for SPY (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 1288 rows for SPY - 15min.\n",
            "   Attempt 1/3: Fetching 60min for SPY (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 337 rows for SPY - 60min.\n",
            "   Attempt 1/3: Fetching DAILY for SPY (TIME_SERIES_DAILY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 6461 rows for SPY - DAILY.\n",
            "   Attempt 1/3: Fetching WEEKLY for SPY (TIME_SERIES_WEEKLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 1340 rows for SPY - WEEKLY.\n",
            "   Attempt 1/3: Fetching MONTHLY for SPY (TIME_SERIES_MONTHLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 308 rows for SPY - MONTHLY.\n",
            " Phase 2: Resampling Higher Timeframes for SPY\n",
            " Phase 3: Labeling Candles for SPY\n",
            " Phase 4: Generating Charts for SPY\n",
            "      Generating chart for SPY-15min...\n",
            "      Generating chart for SPY-60min...\n",
            "      Generating chart for SPY-Daily...\n",
            "      Generating chart for SPY-Weekly...\n",
            "      Generating chart for SPY-Monthly...\n",
            "      Generating chart for SPY-Quarterly...\n",
            "      Generating chart for SPY-Yearly...\n",
            " Phase 5: Analyzing FTFC Reversals for SPY\n",
            "   Analyzing Symbol: SPY for FTFC Reversals...\n",
            "         Found 38 potential FTFC reversal points for SPY on 15min.\n",
            "         Found 53 potential FTFC reversal points for SPY on 60min.\n",
            "         Found 256 potential FTFC reversal points for SPY on Daily.\n",
            "         Found 14 potential FTFC reversal points for SPY on Weekly.\n",
            "   Finished analysis for SPY. Found 354 reversals.\n",
            "   Pausing for 1 sec...\n",
            "\n",
            "--- Processing Symbol: QQQ ---\n",
            " Phase 1: Fetching Data for QQQ\n",
            "   Attempt 1/3: Fetching 15min for QQQ (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 1268 rows for QQQ - 15min.\n",
            "   Attempt 1/3: Fetching 60min for QQQ (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 317 rows for QQQ - 60min.\n",
            "   Attempt 1/3: Fetching DAILY for QQQ (TIME_SERIES_DAILY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 6461 rows for QQQ - DAILY.\n",
            "   Attempt 1/3: Fetching WEEKLY for QQQ (TIME_SERIES_WEEKLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 1340 rows for QQQ - WEEKLY.\n",
            "   Attempt 1/3: Fetching MONTHLY for QQQ (TIME_SERIES_MONTHLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 308 rows for QQQ - MONTHLY.\n",
            " Phase 2: Resampling Higher Timeframes for QQQ\n",
            " Phase 3: Labeling Candles for QQQ\n",
            " Phase 4: Generating Charts for QQQ\n",
            "      Generating chart for QQQ-15min...\n",
            "      Generating chart for QQQ-60min...\n",
            "      Generating chart for QQQ-Daily...\n",
            "      Generating chart for QQQ-Weekly...\n",
            "      Generating chart for QQQ-Monthly...\n",
            "      Generating chart for QQQ-Quarterly...\n",
            "      Generating chart for QQQ-Yearly...\n",
            " Phase 5: Analyzing FTFC Reversals for QQQ\n",
            "   Analyzing Symbol: QQQ for FTFC Reversals...\n",
            "         Found 55 potential FTFC reversal points for QQQ on 15min.\n",
            "         Found 36 potential FTFC reversal points for QQQ on 60min.\n",
            "         Found 233 potential FTFC reversal points for QQQ on Daily.\n",
            "         Found 19 potential FTFC reversal points for QQQ on Weekly.\n",
            "   Finished analysis for QQQ. Found 343 reversals.\n",
            "   Pausing for 1 sec...\n",
            "\n",
            "--- Processing Symbol: XLC ---\n",
            " Phase 1: Fetching Data for XLC\n",
            "   Attempt 1/3: Fetching 15min for XLC (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 862 rows for XLC - 15min.\n",
            "   Attempt 1/3: Fetching 60min for XLC (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 297 rows for XLC - 60min.\n",
            "   Attempt 1/3: Fetching DAILY for XLC (TIME_SERIES_DAILY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 1774 rows for XLC - DAILY.\n",
            "   Attempt 1/3: Fetching WEEKLY for XLC (TIME_SERIES_WEEKLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 368 rows for XLC - WEEKLY.\n",
            "   Attempt 1/3: Fetching MONTHLY for XLC (TIME_SERIES_MONTHLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 85 rows for XLC - MONTHLY.\n",
            " Phase 2: Resampling Higher Timeframes for XLC\n",
            " Phase 3: Labeling Candles for XLC\n",
            " Phase 4: Generating Charts for XLC\n",
            "      Generating chart for XLC-15min...\n",
            "      Generating chart for XLC-60min...\n",
            "      Generating chart for XLC-Daily...\n",
            "      Generating chart for XLC-Weekly...\n",
            "      Generating chart for XLC-Monthly...\n",
            "      Generating chart for XLC-Quarterly...\n",
            "      Generating chart for XLC-Yearly...\n",
            " Phase 5: Analyzing FTFC Reversals for XLC\n",
            "   Analyzing Symbol: XLC for FTFC Reversals...\n",
            "         Found 51 potential FTFC reversal points for XLC on 15min.\n",
            "         Found 37 potential FTFC reversal points for XLC on 60min.\n",
            "         Found 82 potential FTFC reversal points for XLC on Daily.\n",
            "         Found 3 potential FTFC reversal points for XLC on Weekly.\n",
            "   Finished analysis for XLC. Found 172 reversals.\n",
            "   Pausing for 1 sec...\n",
            "\n",
            "--- Processing Symbol: XLY ---\n",
            " Phase 1: Fetching Data for XLY\n",
            "   Attempt 1/3: Fetching 15min for XLY (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 934 rows for XLY - 15min.\n",
            "   Attempt 1/3: Fetching 60min for XLY (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 309 rows for XLY - 60min.\n",
            "   Attempt 1/3: Fetching DAILY for XLY (TIME_SERIES_DAILY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 6461 rows for XLY - DAILY.\n",
            "   Attempt 1/3: Fetching WEEKLY for XLY (TIME_SERIES_WEEKLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 1340 rows for XLY - WEEKLY.\n",
            "   Attempt 1/3: Fetching MONTHLY for XLY (TIME_SERIES_MONTHLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 308 rows for XLY - MONTHLY.\n",
            " Phase 2: Resampling Higher Timeframes for XLY\n",
            " Phase 3: Labeling Candles for XLY\n",
            " Phase 4: Generating Charts for XLY\n",
            "      Generating chart for XLY-15min...\n",
            "      Generating chart for XLY-60min...\n",
            "      Generating chart for XLY-Daily...\n",
            "      Generating chart for XLY-Weekly...\n",
            "      Generating chart for XLY-Monthly...\n",
            "      Generating chart for XLY-Quarterly...\n",
            "      Generating chart for XLY-Yearly...\n",
            " Phase 5: Analyzing FTFC Reversals for XLY\n",
            "   Analyzing Symbol: XLY for FTFC Reversals...\n",
            "         Found 70 potential FTFC reversal points for XLY on 15min.\n",
            "         Found 72 potential FTFC reversal points for XLY on 60min.\n",
            "         Found 239 potential FTFC reversal points for XLY on Daily.\n",
            "         Found 17 potential FTFC reversal points for XLY on Weekly.\n",
            "   Finished analysis for XLY. Found 390 reversals.\n",
            "   Pausing for 1 sec...\n",
            "\n",
            "--- Processing Symbol: XLP ---\n",
            " Phase 1: Fetching Data for XLP\n",
            "   Attempt 1/3: Fetching 15min for XLP (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 1030 rows for XLP - 15min.\n",
            "   Attempt 1/3: Fetching 60min for XLP (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 331 rows for XLP - 60min.\n",
            "   Attempt 1/3: Fetching DAILY for XLP (TIME_SERIES_DAILY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 6461 rows for XLP - DAILY.\n",
            "   Attempt 1/3: Fetching WEEKLY for XLP (TIME_SERIES_WEEKLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 1340 rows for XLP - WEEKLY.\n",
            "   Attempt 1/3: Fetching MONTHLY for XLP (TIME_SERIES_MONTHLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 308 rows for XLP - MONTHLY.\n",
            " Phase 2: Resampling Higher Timeframes for XLP\n",
            " Phase 3: Labeling Candles for XLP\n",
            " Phase 4: Generating Charts for XLP\n",
            "      Generating chart for XLP-15min...\n",
            "      Generating chart for XLP-60min...\n",
            "      Generating chart for XLP-Daily...\n",
            "      Generating chart for XLP-Weekly...\n",
            "      Generating chart for XLP-Monthly...\n",
            "      Generating chart for XLP-Quarterly...\n",
            "      Generating chart for XLP-Yearly...\n",
            " Phase 5: Analyzing FTFC Reversals for XLP\n",
            "   Analyzing Symbol: XLP for FTFC Reversals...\n",
            "         Found 55 potential FTFC reversal points for XLP on 15min.\n",
            "         Found 214 potential FTFC reversal points for XLP on Daily.\n",
            "         Found 23 potential FTFC reversal points for XLP on Weekly.\n",
            "   Finished analysis for XLP. Found 292 reversals.\n",
            "   Pausing for 1 sec...\n",
            "\n",
            "--- Processing Symbol: XLE ---\n",
            " Phase 1: Fetching Data for XLE\n",
            "   Attempt 1/3: Fetching 15min for XLE (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 1272 rows for XLE - 15min.\n",
            "   Attempt 1/3: Fetching 60min for XLE (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 337 rows for XLE - 60min.\n",
            "   Attempt 1/3: Fetching DAILY for XLE (TIME_SERIES_DAILY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 6461 rows for XLE - DAILY.\n",
            "   Attempt 1/3: Fetching WEEKLY for XLE (TIME_SERIES_WEEKLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 1340 rows for XLE - WEEKLY.\n",
            "   Attempt 1/3: Fetching MONTHLY for XLE (TIME_SERIES_MONTHLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 308 rows for XLE - MONTHLY.\n",
            " Phase 2: Resampling Higher Timeframes for XLE\n",
            " Phase 3: Labeling Candles for XLE\n",
            " Phase 4: Generating Charts for XLE\n",
            "      Generating chart for XLE-15min...\n",
            "      Generating chart for XLE-60min...\n",
            "      Generating chart for XLE-Daily...\n",
            "      Generating chart for XLE-Weekly...\n",
            "      Generating chart for XLE-Monthly...\n",
            "      Generating chart for XLE-Quarterly...\n",
            "      Generating chart for XLE-Yearly...\n",
            " Phase 5: Analyzing FTFC Reversals for XLE\n",
            "   Analyzing Symbol: XLE for FTFC Reversals...\n",
            "         Found 94 potential FTFC reversal points for XLE on 15min.\n",
            "         Found 186 potential FTFC reversal points for XLE on Daily.\n",
            "         Found 10 potential FTFC reversal points for XLE on Weekly.\n",
            "   Finished analysis for XLE. Found 290 reversals.\n",
            "   Pausing for 1 sec...\n",
            "\n",
            "--- Processing Symbol: XLF ---\n",
            " Phase 1: Fetching Data for XLF\n",
            "   Attempt 1/3: Fetching 15min for XLF (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 1280 rows for XLF - 15min.\n",
            "   Attempt 1/3: Fetching 60min for XLF (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 337 rows for XLF - 60min.\n",
            "   Attempt 1/3: Fetching DAILY for XLF (TIME_SERIES_DAILY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 6461 rows for XLF - DAILY.\n",
            "   Attempt 1/3: Fetching WEEKLY for XLF (TIME_SERIES_WEEKLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 1340 rows for XLF - WEEKLY.\n",
            "   Attempt 1/3: Fetching MONTHLY for XLF (TIME_SERIES_MONTHLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 308 rows for XLF - MONTHLY.\n",
            " Phase 2: Resampling Higher Timeframes for XLF\n",
            " Phase 3: Labeling Candles for XLF\n",
            " Phase 4: Generating Charts for XLF\n",
            "      Generating chart for XLF-15min...\n",
            "      Generating chart for XLF-60min...\n",
            "      Generating chart for XLF-Daily...\n",
            "      Generating chart for XLF-Weekly...\n",
            "      Generating chart for XLF-Monthly...\n",
            "      Generating chart for XLF-Quarterly...\n",
            "      Generating chart for XLF-Yearly...\n",
            " Phase 5: Analyzing FTFC Reversals for XLF\n",
            "   Analyzing Symbol: XLF for FTFC Reversals...\n",
            "         Found 53 potential FTFC reversal points for XLF on 15min.\n",
            "         Found 5 potential FTFC reversal points for XLF on 60min.\n",
            "         Found 212 potential FTFC reversal points for XLF on Daily.\n",
            "         Found 11 potential FTFC reversal points for XLF on Weekly.\n",
            "   Finished analysis for XLF. Found 281 reversals.\n",
            "   Pausing for 1 sec...\n",
            "\n",
            "--- Processing Symbol: XLV ---\n",
            " Phase 1: Fetching Data for XLV\n",
            "   Attempt 1/3: Fetching 15min for XLV (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 1222 rows for XLV - 15min.\n",
            "   Attempt 1/3: Fetching 60min for XLV (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 336 rows for XLV - 60min.\n",
            "   Attempt 1/3: Fetching DAILY for XLV (TIME_SERIES_DAILY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 6461 rows for XLV - DAILY.\n",
            "   Attempt 1/3: Fetching WEEKLY for XLV (TIME_SERIES_WEEKLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 1340 rows for XLV - WEEKLY.\n",
            "   Attempt 1/3: Fetching MONTHLY for XLV (TIME_SERIES_MONTHLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 308 rows for XLV - MONTHLY.\n",
            " Phase 2: Resampling Higher Timeframes for XLV\n",
            " Phase 3: Labeling Candles for XLV\n",
            " Phase 4: Generating Charts for XLV\n",
            "      Generating chart for XLV-15min...\n",
            "      Generating chart for XLV-60min...\n",
            "      Generating chart for XLV-Daily...\n",
            "      Generating chart for XLV-Weekly...\n",
            "      Generating chart for XLV-Monthly...\n",
            "      Generating chart for XLV-Quarterly...\n",
            "      Generating chart for XLV-Yearly...\n",
            " Phase 5: Analyzing FTFC Reversals for XLV\n",
            "   Analyzing Symbol: XLV for FTFC Reversals...\n",
            "         Found 70 potential FTFC reversal points for XLV on 15min.\n",
            "         Found 36 potential FTFC reversal points for XLV on 60min.\n",
            "         Found 159 potential FTFC reversal points for XLV on Daily.\n",
            "         Found 7 potential FTFC reversal points for XLV on Weekly.\n",
            "   Finished analysis for XLV. Found 272 reversals.\n",
            "   Pausing for 1 sec...\n",
            "\n",
            "--- Processing Symbol: XLI ---\n",
            " Phase 1: Fetching Data for XLI\n",
            "   Attempt 1/3: Fetching 15min for XLI (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 983 rows for XLI - 15min.\n",
            "   Attempt 1/3: Fetching 60min for XLI (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 319 rows for XLI - 60min.\n",
            "   Attempt 1/3: Fetching DAILY for XLI (TIME_SERIES_DAILY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 6461 rows for XLI - DAILY.\n",
            "   Attempt 1/3: Fetching WEEKLY for XLI (TIME_SERIES_WEEKLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 1340 rows for XLI - WEEKLY.\n",
            "   Attempt 1/3: Fetching MONTHLY for XLI (TIME_SERIES_MONTHLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 308 rows for XLI - MONTHLY.\n",
            " Phase 2: Resampling Higher Timeframes for XLI\n",
            " Phase 3: Labeling Candles for XLI\n",
            " Phase 4: Generating Charts for XLI\n",
            "      Generating chart for XLI-15min...\n",
            "      Generating chart for XLI-60min...\n",
            "      Generating chart for XLI-Daily...\n",
            "      Generating chart for XLI-Weekly...\n",
            "      Generating chart for XLI-Monthly...\n",
            "      Generating chart for XLI-Quarterly...\n",
            "      Generating chart for XLI-Yearly...\n",
            " Phase 5: Analyzing FTFC Reversals for XLI\n",
            "   Analyzing Symbol: XLI for FTFC Reversals...\n",
            "         Found 43 potential FTFC reversal points for XLI on 15min.\n",
            "         Found 27 potential FTFC reversal points for XLI on 60min.\n",
            "         Found 261 potential FTFC reversal points for XLI on Daily.\n",
            "         Found 15 potential FTFC reversal points for XLI on Weekly.\n",
            "   Finished analysis for XLI. Found 340 reversals.\n",
            "   Pausing for 1 sec...\n",
            "\n",
            "--- Processing Symbol: XLB ---\n",
            " Phase 1: Fetching Data for XLB\n",
            "   Attempt 1/3: Fetching 15min for XLB (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 821 rows for XLB - 15min.\n",
            "   Attempt 1/3: Fetching 60min for XLB (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 282 rows for XLB - 60min.\n",
            "   Attempt 1/3: Fetching DAILY for XLB (TIME_SERIES_DAILY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 6461 rows for XLB - DAILY.\n",
            "   Attempt 1/3: Fetching WEEKLY for XLB (TIME_SERIES_WEEKLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 1340 rows for XLB - WEEKLY.\n",
            "   Attempt 1/3: Fetching MONTHLY for XLB (TIME_SERIES_MONTHLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 308 rows for XLB - MONTHLY.\n",
            " Phase 2: Resampling Higher Timeframes for XLB\n",
            " Phase 3: Labeling Candles for XLB\n",
            " Phase 4: Generating Charts for XLB\n",
            "      Generating chart for XLB-15min...\n",
            "      Generating chart for XLB-60min...\n",
            "      Generating chart for XLB-Daily...\n",
            "      Generating chart for XLB-Weekly...\n",
            "      Generating chart for XLB-Monthly...\n",
            "      Generating chart for XLB-Quarterly...\n",
            "      Generating chart for XLB-Yearly...\n",
            " Phase 5: Analyzing FTFC Reversals for XLB\n",
            "   Analyzing Symbol: XLB for FTFC Reversals...\n",
            "         Found 55 potential FTFC reversal points for XLB on 15min.\n",
            "         Found 43 potential FTFC reversal points for XLB on 60min.\n",
            "         Found 287 potential FTFC reversal points for XLB on Daily.\n",
            "         Found 16 potential FTFC reversal points for XLB on Weekly.\n",
            "   Finished analysis for XLB. Found 394 reversals.\n",
            "   Pausing for 1 sec...\n",
            "\n",
            "--- Processing Symbol: XLRE ---\n",
            " Phase 1: Fetching Data for XLRE\n",
            "   Attempt 1/3: Fetching 15min for XLRE (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 858 rows for XLRE - 15min.\n",
            "   Attempt 1/3: Fetching 60min for XLRE (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 290 rows for XLRE - 60min.\n",
            "   Attempt 1/3: Fetching DAILY for XLRE (TIME_SERIES_DAILY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 2452 rows for XLRE - DAILY.\n",
            "   Attempt 1/3: Fetching WEEKLY for XLRE (TIME_SERIES_WEEKLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 509 rows for XLRE - WEEKLY.\n",
            "   Attempt 1/3: Fetching MONTHLY for XLRE (TIME_SERIES_MONTHLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 117 rows for XLRE - MONTHLY.\n",
            " Phase 2: Resampling Higher Timeframes for XLRE\n",
            " Phase 3: Labeling Candles for XLRE\n",
            " Phase 4: Generating Charts for XLRE\n",
            "      Generating chart for XLRE-15min...\n",
            "      Generating chart for XLRE-60min...\n",
            "      Generating chart for XLRE-Daily...\n",
            "      Generating chart for XLRE-Weekly...\n",
            "      Generating chart for XLRE-Monthly...\n",
            "      Generating chart for XLRE-Quarterly...\n",
            "      Generating chart for XLRE-Yearly...\n",
            " Phase 5: Analyzing FTFC Reversals for XLRE\n",
            "   Analyzing Symbol: XLRE for FTFC Reversals...\n",
            "         Found 20 potential FTFC reversal points for XLRE on 15min.\n",
            "         Found 58 potential FTFC reversal points for XLRE on Daily.\n",
            "   Finished analysis for XLRE. Found 76 reversals.\n",
            "   Pausing for 1 sec...\n",
            "\n",
            "--- Processing Symbol: XLK ---\n",
            " Phase 1: Fetching Data for XLK\n",
            "   Attempt 1/3: Fetching 15min for XLK (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 1265 rows for XLK - 15min.\n",
            "   Attempt 1/3: Fetching 60min for XLK (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 337 rows for XLK - 60min.\n",
            "   Attempt 1/3: Fetching DAILY for XLK (TIME_SERIES_DAILY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 6461 rows for XLK - DAILY.\n",
            "   Attempt 1/3: Fetching WEEKLY for XLK (TIME_SERIES_WEEKLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 1340 rows for XLK - WEEKLY.\n",
            "   Attempt 1/3: Fetching MONTHLY for XLK (TIME_SERIES_MONTHLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 308 rows for XLK - MONTHLY.\n",
            " Phase 2: Resampling Higher Timeframes for XLK\n",
            " Phase 3: Labeling Candles for XLK\n",
            " Phase 4: Generating Charts for XLK\n",
            "      Generating chart for XLK-15min...\n",
            "      Generating chart for XLK-60min...\n",
            "      Generating chart for XLK-Daily...\n",
            "      Generating chart for XLK-Weekly...\n",
            "      Generating chart for XLK-Monthly...\n",
            "      Generating chart for XLK-Quarterly...\n",
            "      Generating chart for XLK-Yearly...\n",
            " Phase 5: Analyzing FTFC Reversals for XLK\n",
            "   Analyzing Symbol: XLK for FTFC Reversals...\n",
            "         Found 98 potential FTFC reversal points for XLK on 15min.\n",
            "         Found 95 potential FTFC reversal points for XLK on 60min.\n",
            "         Found 217 potential FTFC reversal points for XLK on Daily.\n",
            "         Found 15 potential FTFC reversal points for XLK on Weekly.\n",
            "   Finished analysis for XLK. Found 425 reversals.\n",
            "   Pausing for 1 sec...\n",
            "\n",
            "--- Processing Symbol: XLU ---\n",
            " Phase 1: Fetching Data for XLU\n",
            "   Attempt 1/3: Fetching 15min for XLU (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 1068 rows for XLU - 15min.\n",
            "   Attempt 1/3: Fetching 60min for XLU (TIME_SERIES_INTRADAY)...\n",
            "   OK: Fetched/cleaned 326 rows for XLU - 60min.\n",
            "   Attempt 1/3: Fetching DAILY for XLU (TIME_SERIES_DAILY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 6461 rows for XLU - DAILY.\n",
            "   Attempt 1/3: Fetching WEEKLY for XLU (TIME_SERIES_WEEKLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 1340 rows for XLU - WEEKLY.\n",
            "   Attempt 1/3: Fetching MONTHLY for XLU (TIME_SERIES_MONTHLY_ADJUSTED)...\n",
            "   OK: Fetched/cleaned 308 rows for XLU - MONTHLY.\n",
            " Phase 2: Resampling Higher Timeframes for XLU\n",
            " Phase 3: Labeling Candles for XLU\n",
            " Phase 4: Generating Charts for XLU\n",
            "      Generating chart for XLU-15min...\n",
            "      Generating chart for XLU-60min...\n",
            "      Generating chart for XLU-Daily...\n",
            "      Generating chart for XLU-Weekly...\n",
            "      Generating chart for XLU-Monthly...\n",
            "      Generating chart for XLU-Quarterly...\n",
            "      Generating chart for XLU-Yearly...\n",
            " Phase 5: Analyzing FTFC Reversals for XLU\n",
            "   Analyzing Symbol: XLU for FTFC Reversals...\n",
            "         Found 40 potential FTFC reversal points for XLU on 15min.\n",
            "         Found 41 potential FTFC reversal points for XLU on 60min.\n",
            "         Found 198 potential FTFC reversal points for XLU on Daily.\n",
            "         Found 16 potential FTFC reversal points for XLU on Weekly.\n",
            "   Finished analysis for XLU. Found 295 reversals.\n",
            "   Pausing for 1 sec...\n",
            "\n",
            "--- Finished Processing All Symbols. Successfully processed: ['SPY', 'QQQ', 'XLC', 'XLY', 'XLP', 'XLE', 'XLF', 'XLV', 'XLI', 'XLB', 'XLRE', 'XLK', 'XLU'] ---\n",
            "Combining detailed results...\n",
            "Combined results shape: (3924, 61)\n",
            "\n",
            "--- Aggregating Combined Performance Summary ---\n",
            "   Aggregating performance results...\n",
            "   Aggregation complete. Summary has 359 rows.\n",
            "\n",
            "--- Saving Combined Detailed Performance Results ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1-338636654.py:639: DeprecationWarning:\n",
            "\n",
            "is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Combined detailed results saved to: 2025-07-11_FTFC_Reversal_Performance_Detailed_ALL.xlsx\n",
            "   Combined detailed results saved to: 2025-07-11_FTFC_Reversal_Performance_Detailed_ALL.json\n",
            "\n",
            "--- Saving Combined Performance Summary ---\n",
            "   Combined aggregated performance summary saved to: 2025-07-11_FTFC_Reversal_Performance_Summary_ALL.xlsx\n",
            "   Combined aggregated performance summary saved to: 2025-07-11_FTFC_Reversal_Performance_Summary_ALL.json\n",
            "\n",
            "--- Analysis Complete at 2025-07-11 12:54:37 ---\n",
            "Total execution time: 0:02:18.700105\n"
          ]
        }
      ],
      "source": [
        "!pip install alpha_vantage pandas openpyxl plotly kaleido PyPDF2\n",
        "!pip install google-colab # Ensure google.colab is available if not in a standard Colab env\n",
        "\n",
        "import json\n",
        "try:\n",
        "    from google.colab import userdata # Import userdata for Colab secrets\n",
        "    COLAB_ENV = True\n",
        "except ImportError:\n",
        "    COLAB_ENV = False # Not running in Colab or userdata not available\n",
        "    print(\"Warning: google.colab.userdata not found. Falling back to config file method for API key.\")\n",
        "\n",
        "from alpha_vantage.timeseries import TimeSeries\n",
        "import pandas as pd\n",
        "from datetime import date, datetime, timedelta\n",
        "import os\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "# from PyPDF2 import PdfMerger # No longer merging PDFs, saving HTML instead\n",
        "import tempfile\n",
        "import warnings\n",
        "import time # Import time for sleep delays\n",
        "\n",
        "# Suppress specific warnings if they become noisy (e.g., from pandas)\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "pd.options.mode.chained_assignment = None # Suppress SettingWithCopyWarning\n",
        "\n",
        "\n",
        "# ===============\n",
        "#  CONFIGURATION\n",
        "# ===============\n",
        "# --- User Configuration ---\n",
        "CONFIG_FILE_PATH = \"config.json\"\n",
        "SYMBOLS = ['SPY', 'QQQ', 'XLC', 'XLY', 'XLP', 'XLE', 'XLF', 'XLV',\n",
        "           'XLI', 'XLB', 'XLRE', 'XLK', 'XLU']\n",
        "PERFORMANCE_LOOKAHEAD = 10\n",
        "MIN_HIGHER_TFS_FOR_FTFC = 3\n",
        "COLAB_SECRET_NAME = 'API_Key'\n",
        "SLEEP_BETWEEN_SYMBOLS = 1 # seconds\n",
        "# --- Charting Configuration ---\n",
        "CREATE_CHARTS = True # Set to False to disable chart generation\n",
        "CHART_OUTPUT_DIR = \"charts\" # Subdirectory to save HTML charts\n",
        "# --- End User Configuration ---\n",
        "\n",
        "\n",
        "# Alpha Vantage API function mapping and timeframe processing order\n",
        "TIMEFRAMES_API = {\n",
        "    'TIME_SERIES_INTRADAY': ['15min', '60min'],\n",
        "    'TIME_SERIES_DAILY_ADJUSTED': 'Daily',\n",
        "    'TIME_SERIES_WEEKLY_ADJUSTED': 'Weekly',\n",
        "    'TIME_SERIES_MONTHLY_ADJUSTED': 'Monthly'\n",
        "}\n",
        "TIMEFRAME_ORDER = ['15min', '60min', 'Daily', 'Weekly', 'Monthly', 'Quarterly', 'Yearly']\n",
        "\n",
        "# --- MODIFIED: Define Strat patterns as tuples for easier checking ---\n",
        "REVERSAL_PATTERNS_STRAT = {\n",
        "    # 3-Bar Patterns\n",
        "    (\"3\", \"1\", \"2u\"): \"3-1-2u\",\n",
        "    (\"3\", \"1\", \"2d\"): \"3-1-2d\",\n",
        "    (\"2u\", \"1\", \"2d\"): \"2u-1-2d\",\n",
        "    (\"2d\", \"1\", \"2u\"): \"2d-1-2u\",\n",
        "    # 2-Bar Patterns (ensure they don't overlap with ends of 3-bar)\n",
        "    (\"2u\", \"2d\"): \"2u-2d\",\n",
        "    (\"2d\", \"2u\"): \"2d-2u\",\n",
        "}\n",
        "\n",
        "# --- Load API Key ---\n",
        "API_KEY = None\n",
        "if COLAB_ENV:\n",
        "    try: API_KEY = userdata.get(COLAB_SECRET_NAME); print(f\"OK: API Key from Colab '{COLAB_SECRET_NAME}'.\")\n",
        "    except Exception as e: print(f\"ERROR: Colab secret '{COLAB_SECRET_NAME}' not found or error: {e}\"); exit()\n",
        "else:\n",
        "    try:\n",
        "        with open(CONFIG_FILE_PATH, 'r') as f: config = json.load(f)\n",
        "        API_KEY = config.get('alpha_vantage', {}).get('api_key')\n",
        "        if API_KEY: print(f\"OK: API Key from {CONFIG_FILE_PATH}.\")\n",
        "        else: print(f\"ERROR: API Key not found in {CONFIG_FILE_PATH}.\"); exit()\n",
        "    except Exception as e: print(f\"ERROR: Loading API Key from {CONFIG_FILE_PATH}: {e}\"); exit()\n",
        "if not API_KEY: print(\"ERROR: API_KEY not loaded.\"); exit()\n",
        "\n",
        "# Initialize Alpha Vantage client\n",
        "try: ts = TimeSeries(key=API_KEY, output_format='pandas'); print(\"OK: Alpha Vantage client initialized.\")\n",
        "except Exception as e: print(f\"ERROR: Initializing Alpha Vantage client: {e}\"); exit()\n",
        "\n",
        "\n",
        "# ================\n",
        "#  DATA FETCHING\n",
        "# ================\n",
        "def get_data(symbol, function, interval=None):\n",
        "    \"\"\" Fetches and cleans time series data from Alpha Vantage. \"\"\"\n",
        "    max_retries = 3; retry_delay = 10\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            tf_name = interval or function.split('TIME_SERIES_')[-1].replace('_ADJUSTED','')\n",
        "            print(f\"   Attempt {attempt+1}/{max_retries}: Fetching {tf_name} for {symbol} ({function})...\")\n",
        "            # API Call (omitted specific calls for brevity - same as previous version)\n",
        "            if function == 'TIME_SERIES_INTRADAY': df, _ = ts.get_intraday(symbol=symbol, interval=interval, outputsize='full', extended_hours=False)\n",
        "            elif function == 'TIME_SERIES_DAILY': df, _ = ts.get_daily(symbol=symbol, outputsize='full')\n",
        "            elif function == 'TIME_SERIES_DAILY_ADJUSTED': df, _ = ts.get_daily_adjusted(symbol=symbol, outputsize='full')\n",
        "            elif function == 'TIME_SERIES_WEEKLY': df, _ = ts.get_weekly(symbol=symbol)\n",
        "            elif function == 'TIME_SERIES_WEEKLY_ADJUSTED': df, _ = ts.get_weekly_adjusted(symbol=symbol)\n",
        "            elif function == 'TIME_SERIES_MONTHLY': df, _ = ts.get_monthly(symbol=symbol)\n",
        "            elif function == 'TIME_SERIES_MONTHLY_ADJUSTED': df, _ = ts.get_monthly_adjusted(symbol=symbol)\n",
        "            else: print(f\"Warning: Unknown function '{function}'.\"); return None\n",
        "\n",
        "            if df is not None and not df.empty:\n",
        "                # --- Data Cleaning ---\n",
        "                df.columns = [c.split('. ')[-1].replace(' ', '_') if '.' in c else c.replace(' ', '_') for c in df.columns]\n",
        "                try: df.index = pd.to_datetime(df.index, errors='coerce')\n",
        "                except Exception as e: print(f\"   ERROR converting index: {e}.\"); return None\n",
        "                df = df[pd.notna(df.index)] # Drop NaT indices\n",
        "                if not isinstance(df.index, pd.DatetimeIndex) or df.index.empty: print(f\"   WARNING: Invalid DatetimeIndex after conversion.\"); return None\n",
        "                df = df.iloc[::-1] # Oldest first\n",
        "                ohlcv_cols = ['open', 'high', 'low', 'close', 'adjusted_close', 'volume', 'dividend_amount', 'split_coefficient']\n",
        "                for col in ohlcv_cols:\n",
        "                   if col in df.columns:\n",
        "                       if not pd.api.types.is_numeric_dtype(df[col]): df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "                       if np.isinf(df[col]).any(): df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
        "                essential_ohlc = ['open', 'high', 'low', 'close']\n",
        "                if 'ADJUSTED' in function and 'adjusted_close' in df.columns: essential_ohlc.append('adjusted_close')\n",
        "                df.dropna(subset=essential_ohlc, inplace=True)\n",
        "                if df.empty: return None\n",
        "                print(f\"   OK: Fetched/cleaned {len(df)} rows for {symbol} - {tf_name}.\")\n",
        "                return df\n",
        "            else: time.sleep(1); return None # No data returned\n",
        "        except ValueError as ve: # Handle API errors\n",
        "             print(f\"   ERROR (ValueError): {ve}\")\n",
        "             if \"rate limit\" in str(ve): time.sleep(retry_delay * (2**attempt)) # Backoff for rate limit\n",
        "             elif \"premium endpoint\" in str(ve).lower() or \"Invalid API call\" in str(ve) or \"invalid symbol\" in str(ve).lower(): return None # Don't retry these\n",
        "             elif attempt == max_retries - 1: return None # Max retries for other ValueErrors\n",
        "             else: time.sleep(retry_delay * (2**attempt))\n",
        "        except Exception as e: # Handle other errors\n",
        "            print(f\"   ERROR (Unexpected): {type(e).__name__} - {e}\")\n",
        "            if attempt == max_retries - 1: return None\n",
        "            else: time.sleep(retry_delay * (2**attempt))\n",
        "    print(f\"   Failed fetch for {symbol} - {tf_name} after retries.\")\n",
        "    return None\n",
        "\n",
        "# ============================\n",
        "#  CANDLESTICK LABELING\n",
        "# ============================\n",
        "def label_candlesticks(df):\n",
        "    \"\"\" Applies 'The Strat' labels (1, 2u, 2d, 3). Assumes sorted DatetimeIndex. \"\"\"\n",
        "    if df is None or df.empty: return df\n",
        "    if not isinstance(df.index, pd.DatetimeIndex): df['label'] = 'N/A'; return df\n",
        "    if not df.index.is_monotonic_increasing: df = df.sort_index()\n",
        "    if not all(col in df.columns for col in ['high', 'low']): df['label'] = 'N/A'; return df\n",
        "    if not pd.api.types.is_numeric_dtype(df['high']) or not pd.api.types.is_numeric_dtype(df['low']):\n",
        "         df['high']=pd.to_numeric(df['high'], errors='coerce'); df['low']=pd.to_numeric(df['low'], errors='coerce'); df.dropna(subset=['high', 'low'], inplace=True)\n",
        "         if df.empty: df['label'] = 'N/A'; return df\n",
        "    prev_high = df['high'].shift(1); prev_low = df['low'].shift(1); curr_high = df['high']; curr_low = df['low']\n",
        "    is_inside = (curr_high <= prev_high) & (curr_low >= prev_low); is_up = (curr_high > prev_high) & (curr_low >= prev_low)\n",
        "    is_down = (curr_high <= prev_high) & (curr_low < prev_low); is_outside = (curr_high > prev_high) & (curr_low < prev_low)\n",
        "    df['label'] = 'N/A'; df.loc[is_inside, 'label'] = '1'; df.loc[is_up, 'label'] = '2u'; df.loc[is_down, 'label'] = '2d'; df.loc[is_outside, 'label'] = '3'\n",
        "    if not df.empty: df.iloc[0, df.columns.get_loc('label')] = 'N/A'\n",
        "    return df\n",
        "\n",
        "# ============================\n",
        "#  HAMMER / SHOOTER DETECTION\n",
        "# ============================\n",
        "def add_hammer_shooter_columns(df):\n",
        "    \"\"\" Adds boolean 'is_hammer'/'is_shooter' columns. \"\"\"\n",
        "    if df is None or df.empty: return df\n",
        "    required_cols = ['open', 'high', 'low', 'close']\n",
        "    if not all(col in df.columns for col in required_cols): df['is_hammer']=False; df['is_shooter']=False; return df\n",
        "    for col in required_cols:\n",
        "         if not pd.api.types.is_numeric_dtype(df[col]): df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    df.dropna(subset=required_cols, inplace=True)\n",
        "    if df.empty: df['is_hammer']=False; df['is_shooter']=False; return df\n",
        "    body_size = (df['close'] - df['open']).abs().replace(0, 0.00001); candle_range = (df['high'] - df['low']).replace(0, 0.00001)\n",
        "    upper_wick = df['high'] - df[['open', 'close']].max(axis=1); lower_wick = df[['open', 'close']].min(axis=1) - df['low']\n",
        "    body_midpoint = (df['close'] + df['open']) / 2; candle_lower_half_top = df['low'] + 0.5 * candle_range; candle_upper_half_bottom = df['high'] - 0.5 * candle_range\n",
        "    is_hammer = ( (lower_wick >= 1.9 * body_size) & (upper_wick < body_size * 0.8) & (body_midpoint >= candle_upper_half_bottom) )\n",
        "    is_shooter = ( (upper_wick >= 1.9 * body_size) & (lower_wick < body_size * 0.8) & (body_midpoint <= candle_lower_half_top) )\n",
        "    df.loc[:, 'is_hammer'] = is_hammer; df.loc[:, 'is_shooter'] = is_shooter\n",
        "    return df\n",
        "\n",
        "# ==================\n",
        "#  DATA RESAMPLING\n",
        "# ==================\n",
        "def resample_data(df, freq='QE'):\n",
        "    \"\"\" Resamples OHLCV data to a lower frequency (e.g., Quarterly, Yearly). \"\"\"\n",
        "    if df is None or df.empty: return None\n",
        "    if not isinstance(df.index, pd.DatetimeIndex): return None\n",
        "    if not df.index.is_monotonic_increasing: df = df.sort_index()\n",
        "    agg_dict = {'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last'}\n",
        "    if 'volume' in df.columns and pd.api.types.is_numeric_dtype(df['volume']): agg_dict['volume'] = 'sum'\n",
        "    close_col_to_use = 'adjusted_close' if 'adjusted_close' in df.columns and pd.api.types.is_numeric_dtype(df['adjusted_close']) else 'close'\n",
        "    agg_dict[close_col_to_use] = 'last'\n",
        "    if close_col_to_use == 'adjusted_close' and 'close' in df.columns: agg_dict['close'] = 'last'\n",
        "    if 'dividend_amount' in df.columns and pd.api.types.is_numeric_dtype(df['dividend_amount']): agg_dict['dividend_amount'] = 'sum'\n",
        "    if 'split_coefficient' in df.columns and pd.api.types.is_numeric_dtype(df['split_coefficient']): agg_dict['split_coefficient'] = 'prod'\n",
        "    try:\n",
        "        df_resampled = df.resample(freq).agg(agg_dict)\n",
        "        df_resampled.dropna(subset=['open', 'high', 'low', 'close'], how='all', inplace=True)\n",
        "        if df_resampled.empty: return None\n",
        "        if not isinstance(df_resampled.index, pd.DatetimeIndex): return None\n",
        "        if close_col_to_use == 'adjusted_close' and 'adjusted_close' in df_resampled.columns:\n",
        "             if 'close' in df_resampled.columns: df_resampled['close'] = df_resampled['adjusted_close']; df_resampled.drop(columns=['adjusted_close'], inplace=True, errors='ignore')\n",
        "             else: df_resampled.rename(columns={'adjusted_close': 'close'}, inplace=True)\n",
        "        df_resampled = label_candlesticks(df_resampled)\n",
        "        df_resampled = add_hammer_shooter_columns(df_resampled)\n",
        "        return df_resampled\n",
        "    except Exception as e: print(f\"Error during resampling to '{freq}': {e}\"); return None\n",
        "\n",
        "# ===================================================\n",
        "#   HISTORICAL FTFC REVERSAL ANALYSIS\n",
        "# ===================================================\n",
        "def analyze_historical_ftfc_reversals(symbol_data, symbol, timeframe_order, lookahead=10, min_higher_tfs=3):\n",
        "    \"\"\"\n",
        "    Analyzes historical data FOR A SINGLE SYMBOL to find FTFC reversals,\n",
        "    identifies the Strat pattern, and calculates performance.\n",
        "    \"\"\"\n",
        "    print(f\"   Analyzing Symbol: {symbol} for FTFC Reversals...\")\n",
        "    historical_reversals = []\n",
        "\n",
        "    # --- Precompute aligned labels ---\n",
        "    aligned_labels_cache = {}\n",
        "    available_tfs_info = []\n",
        "    for tf in timeframe_order:\n",
        "         df = symbol_data.get(tf)\n",
        "         if df is not None and not df.empty and isinstance(df.index, pd.DatetimeIndex):\n",
        "             if 'label' in df.columns: available_tfs_info.append({'name': tf, 'index': df.index, 'labels': df['label']})\n",
        "    if not available_tfs_info: return pd.DataFrame()\n",
        "    all_valid_indices = pd.Index([])\n",
        "    if available_tfs_info:\n",
        "        all_valid_indices = available_tfs_info[0]['index']\n",
        "        for i in range(1, len(available_tfs_info)):\n",
        "            try:\n",
        "                if isinstance(all_valid_indices, pd.DatetimeIndex) and isinstance(available_tfs_info[i]['index'], pd.DatetimeIndex):\n",
        "                     all_valid_indices = all_valid_indices.union(available_tfs_info[i]['index'])\n",
        "            except Exception as e: print(f\"      Error during index union for {symbol} at index {i}: {e}\")\n",
        "    if all_valid_indices.empty: return pd.DataFrame()\n",
        "    if not isinstance(all_valid_indices, pd.DatetimeIndex):\n",
        "         all_valid_indices = pd.to_datetime(all_valid_indices, errors='coerce').dropna()\n",
        "         if not isinstance(all_valid_indices, pd.DatetimeIndex) or all_valid_indices.empty: return pd.DataFrame()\n",
        "    base_aligned_df = pd.DataFrame(index=all_valid_indices.sort_values())\n",
        "    for tf_info in available_tfs_info:\n",
        "        tf_name = tf_info['name']; label_series = tf_info['labels']\n",
        "        if label_series is not None and isinstance(label_series.index, pd.DatetimeIndex):\n",
        "            if isinstance(base_aligned_df.index, pd.DatetimeIndex):\n",
        "                try:\n",
        "                    if not label_series.index.is_monotonic_increasing: label_series = label_series.sort_index()\n",
        "                    temp_aligned = pd.merge_asof(\n",
        "                        base_aligned_df, label_series.rename(f'label_{tf_name}'),\n",
        "                        left_index=True, right_index=True, direction='backward', tolerance=pd.Timedelta('30 days')\n",
        "                    )\n",
        "                    aligned_labels_cache[tf_name] = temp_aligned[f'label_{tf_name}']\n",
        "                except Exception as e: print(f\"      ERROR during merge_asof for {symbol}-{tf_name}: {e}. Skipping.\")\n",
        "\n",
        "    # --- Iterate through timeframes to find reversals ---\n",
        "    available_tfs_with_aligned_labels = sorted(\n",
        "         [tf for tf in timeframe_order if tf in aligned_labels_cache], key=lambda x: timeframe_order.index(x)\n",
        "    )\n",
        "    if not available_tfs_with_aligned_labels: return pd.DataFrame()\n",
        "\n",
        "    for i, smaller_tf in enumerate(available_tfs_with_aligned_labels):\n",
        "        smaller_df = symbol_data.get(smaller_tf)\n",
        "        if smaller_df is None or smaller_df.empty or not isinstance(smaller_df.index, pd.DatetimeIndex): continue\n",
        "\n",
        "        higher_tfs = []\n",
        "        for j in range(i + 1, len(available_tfs_with_aligned_labels)):\n",
        "            higher_tfs.append(available_tfs_with_aligned_labels[j]);\n",
        "            if len(higher_tfs) == min_higher_tfs: break\n",
        "        if len(higher_tfs) < min_higher_tfs: continue\n",
        "\n",
        "        try:\n",
        "            aligned_smaller_labels_series = aligned_labels_cache[smaller_tf]\n",
        "            aligned_higher_labels_series_list = [aligned_labels_cache[htf] for htf in higher_tfs]\n",
        "            aligned_smaller_labels = aligned_smaller_labels_series.reindex(smaller_df.index)\n",
        "            aligned_higher_labels_list = [s.reindex(smaller_df.index) for s in aligned_higher_labels_series_list]\n",
        "        except Exception as e: print(f\"      Error accessing/reindexing labels for {symbol} on {smaller_tf}: {e}\"); continue\n",
        "\n",
        "        try:\n",
        "            labels_df = pd.DataFrame({'smaller_label': aligned_smaller_labels, **{f'higher_label_{k}': s for k, s in enumerate(aligned_higher_labels_list)}})\n",
        "        except Exception as e: print(f\"      Error creating labels_df for {symbol}-{smaller_tf}: {e}\"); continue\n",
        "        labels_df.dropna(inplace=True)\n",
        "        if labels_df.empty: continue\n",
        "\n",
        "        required_label_cols = ['smaller_label'] + [f'higher_label_{k}' for k in range(min_higher_tfs)]\n",
        "        if not all(col in labels_df.columns for col in required_label_cols): continue\n",
        "\n",
        "        htf_2u_trend = (labels_df[f'higher_label_0'] == '2u'); htf_2d_trend = (labels_df[f'higher_label_0'] == '2d')\n",
        "        for k in range(1, min_higher_tfs): htf_2u_trend &= (labels_df[f'higher_label_{k}'] == '2u'); htf_2d_trend &= (labels_df[f'higher_label_{k}'] == '2d')\n",
        "        smaller_tf_rev_vs_2u = labels_df['smaller_label'].isin(['2d', '1', '3']); smaller_tf_rev_vs_2d = labels_df['smaller_label'].isin(['2u', '1', '3'])\n",
        "        reversal_bullish_setup = htf_2u_trend & smaller_tf_rev_vs_2u; reversal_bearish_setup = htf_2d_trend & smaller_tf_rev_vs_2d\n",
        "        reversal_indices = labels_df.index[reversal_bullish_setup | reversal_bearish_setup]\n",
        "        if reversal_indices.empty: continue\n",
        "\n",
        "        print(f\"         Found {len(reversal_indices)} potential FTFC reversal points for {symbol} on {smaller_tf}.\")\n",
        "\n",
        "        # --- Analyze Performance & Strat Pattern ---\n",
        "        for reversal_time in reversal_indices:\n",
        "            try:\n",
        "                reversal_iloc = smaller_df.index.get_loc(reversal_time)\n",
        "                if reversal_iloc + 1 + lookahead > len(smaller_df): continue\n",
        "\n",
        "                trend = '2u' if htf_2u_trend.loc[reversal_time] else ('2d' if htf_2d_trend.loc[reversal_time] else 'Mixed')\n",
        "                reversal_label = labels_df.loc[reversal_time, 'smaller_label'] # The single candle label causing FTFC break\n",
        "                higher_tf_labels_at_reversal = {f'Higher TF{k+1} Label': labels_df.loc[reversal_time, f'higher_label_{k}'] for k in range(min_higher_tfs)}\n",
        "\n",
        "                # --- Check for Strat Pattern ---\n",
        "                strat_pattern_found = \"N/A\" # Default if no pattern matches\n",
        "                # Check 3-bar pattern ending at reversal_iloc\n",
        "                if reversal_iloc >= 2:\n",
        "                    label_seq_3 = tuple(smaller_df['label'].iloc[reversal_iloc-2 : reversal_iloc+1])\n",
        "                    strat_pattern_found = REVERSAL_PATTERNS_STRAT.get(label_seq_3, \"N/A\")\n",
        "\n",
        "                # If no 3-bar found, check 2-bar pattern ending at reversal_iloc\n",
        "                if strat_pattern_found == \"N/A\" and reversal_iloc >= 1:\n",
        "                    label_seq_2 = tuple(smaller_df['label'].iloc[reversal_iloc-1 : reversal_iloc+1])\n",
        "                    strat_pattern_found = REVERSAL_PATTERNS_STRAT.get(label_seq_2, \"N/A\")\n",
        "                # --- End Strat Pattern Check ---\n",
        "\n",
        "                reversal_candle = smaller_df.iloc[reversal_iloc]; future_candles = smaller_df.iloc[reversal_iloc + 1 : reversal_iloc + 1 + lookahead]\n",
        "                if future_candles.empty: continue\n",
        "\n",
        "                entry_price_col = 'adjusted_close' if 'adjusted_close' in reversal_candle.index and pd.notna(reversal_candle['adjusted_close']) else 'close'\n",
        "                close_price_col = 'adjusted_close' if 'adjusted_close' in future_candles.columns else 'close'\n",
        "                open_price_col = 'open'\n",
        "                entry_price = reversal_candle[entry_price_col]\n",
        "                if pd.isna(entry_price) or entry_price == 0: continue\n",
        "\n",
        "                perf_data = {'Symbol': symbol, 'Reversal Time': reversal_time, 'Reversal Timeframe': smaller_tf,\n",
        "                             'FTFC Trigger Label': reversal_label, # Renamed original label\n",
        "                             'Strat Pattern': strat_pattern_found, # Added Strat pattern\n",
        "                             'Higher TF Trend': trend, 'Entry Price': entry_price,\n",
        "                             'Higher TFs Used': \", \".join(higher_tfs), **higher_tf_labels_at_reversal}\n",
        "\n",
        "                for k in range(1, lookahead + 1):\n",
        "                    actual_candle_index = k - 1; col_prefix = f'Fwd_{k}'\n",
        "                    if actual_candle_index < len(future_candles):\n",
        "                        candle = future_candles.iloc[actual_candle_index]\n",
        "                        if close_price_col not in candle.index or open_price_col not in candle.index: continue\n",
        "                        current_close = candle[close_price_col]; current_open = candle[open_price_col]\n",
        "                        if pd.isna(current_close) or pd.isna(current_open) or pd.isna(entry_price): gross_move_from_entry, perc_move_from_entry, candle_gross_move, candle_perc_move = np.nan, np.nan, np.nan, np.nan\n",
        "                        else:\n",
        "                            gross_move_from_entry = current_close - entry_price; perc_move_from_entry = (gross_move_from_entry / entry_price) * 100 if entry_price != 0 else 0\n",
        "                            candle_gross_move = current_close - current_open; candle_perc_move = (candle_gross_move / current_open) * 100 if current_open != 0 else 0\n",
        "                        perf_data[f'{col_prefix}_Candle_Close'] = current_close; perf_data[f'{col_prefix}_GrossMoveFromEntry'] = gross_move_from_entry\n",
        "                        perf_data[f'{col_prefix}_PercMoveFromEntry'] = perc_move_from_entry; perf_data[f'{col_prefix}_Candle_GrossMove'] = candle_gross_move\n",
        "                        perf_data[f'{col_prefix}_Candle_PercMove'] = candle_perc_move\n",
        "                    else: perf_data[f'{col_prefix}_Candle_Close']=np.nan; perf_data[f'{col_prefix}_GrossMoveFromEntry']=np.nan; perf_data[f'{col_prefix}_PercMoveFromEntry']=np.nan; perf_data[f'{col_prefix}_Candle_GrossMove']=np.nan; perf_data[f'{col_prefix}_Candle_PercMove']=np.nan\n",
        "                historical_reversals.append(perf_data)\n",
        "            except Exception as e: print(f\"         Unexpected Error processing reversal at {reversal_time} for {symbol} on {smaller_tf}: {type(e).__name__} - {e}\")\n",
        "\n",
        "    if not historical_reversals: return pd.DataFrame()\n",
        "    else:\n",
        "        results_df = pd.DataFrame(historical_reversals)\n",
        "        # --- Update column order for new 'Strat Pattern' ---\n",
        "        id_cols = ['Symbol', 'Reversal Time', 'Reversal Timeframe', 'FTFC Trigger Label', 'Strat Pattern', 'Higher TF Trend', 'Entry Price', 'Higher TFs Used']\n",
        "        label_cols = sorted([col for col in results_df.columns if 'Label' in col and 'FTFC Trigger' not in col]) # Exclude the trigger label here\n",
        "        perf_cols = sorted([col for col in results_df.columns if col.startswith('Fwd_')], key=lambda x: (int(x.split('_')[1]), x))\n",
        "        final_cols = [col for col in id_cols + label_cols + perf_cols if col in results_df.columns]\n",
        "        if 'Reversal Time' in results_df.columns: results_df['Reversal Time'] = pd.to_datetime(results_df['Reversal Time'])\n",
        "        return results_df[final_cols]\n",
        "\n",
        "\n",
        "# ===================================================\n",
        "#  HELPER: SAVE DATAFRAMES TO EXCEL (Optional Data Dump)\n",
        "# ===================================================\n",
        "def save_dataframes_to_excel(symbol, data_dict, date_prefix):\n",
        "    \"\"\" Saves processed dataframes (OHLC, labels) to an Excel file. Optional. \"\"\"\n",
        "    excel_filename = f\"{date_prefix}_{symbol}_SourceData.xlsx\"\n",
        "    try:\n",
        "        with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
        "            for timeframe in timeframe_order:\n",
        "                df_data = data_dict.get(timeframe)\n",
        "                if df_data is not None and not df_data.empty:\n",
        "                    df_copy = df_data.copy()\n",
        "                    if pd.api.types.is_datetime64tz_dtype(df_copy.index): df_copy.index = df_copy.index.tz_localize(None)\n",
        "                    df_copy.to_excel(writer, sheet_name=f\"Data_{timeframe}\")\n",
        "        print(f\"   Source data saved to: {excel_filename}\")\n",
        "    except Exception as e: print(f\"   Error saving source data to Excel for {symbol}: {e}\")\n",
        "\n",
        "\n",
        "# ===================================================\n",
        "#  HELPER: AGGREGATE PERFORMANCE RESULTS\n",
        "# ===================================================\n",
        "def aggregate_performance_results(historical_df):\n",
        "    \"\"\" Aggregates the historical performance results, calculating average moves. \"\"\"\n",
        "    if historical_df is None or historical_df.empty: print(\"   No historical data to aggregate.\"); return pd.DataFrame()\n",
        "    print(\"   Aggregating performance results...\")\n",
        "    gross_move_entry_cols=[col for col in historical_df.columns if col.startswith('Fwd_') and col.endswith('_GrossMoveFromEntry')]\n",
        "    perc_move_entry_cols=[col for col in historical_df.columns if col.startswith('Fwd_') and col.endswith('_PercMoveFromEntry')]\n",
        "    candle_gross_cols=[col for col in historical_df.columns if col.startswith('Fwd_') and col.endswith('_Candle_GrossMove')]\n",
        "    candle_perc_cols=[col for col in historical_df.columns if col.startswith('Fwd_') and col.endswith('_Candle_PercMove')]\n",
        "    all_perf_cols = gross_move_entry_cols + perc_move_entry_cols + candle_gross_cols + candle_perc_cols\n",
        "    # --- Update grouping keys ---\n",
        "    grouping_keys = ['Symbol', 'Reversal Timeframe', 'Higher TF Trend', 'Strat Pattern', 'FTFC Trigger Label']\n",
        "    valid_grouping_keys = [key for key in grouping_keys if key in historical_df.columns]\n",
        "    if not valid_grouping_keys: print(\"   Error: None grouping keys found.\"); return pd.DataFrame()\n",
        "    if len(valid_grouping_keys) < len(grouping_keys): print(f\"   Warning: Missing grouping keys: {set(grouping_keys)-set(valid_grouping_keys)}\")\n",
        "    try:\n",
        "        grouped = historical_df.groupby(valid_grouping_keys)\n",
        "        numeric_perf_cols = historical_df[all_perf_cols].select_dtypes(include=np.number).columns.tolist()\n",
        "        if not numeric_perf_cols: print(\"   Error: No numeric performance columns found.\"); return pd.DataFrame()\n",
        "        summary = grouped[numeric_perf_cols].mean()\n",
        "        summary['Count'] = grouped.size()\n",
        "    except Exception as e: print(f\"   Error during aggregation mean/count: {e}\"); return pd.DataFrame()\n",
        "    summary.columns = [f'Avg_{col}' if col != 'Count' else col for col in summary.columns]\n",
        "    print(f\"   Aggregation complete. Summary has {len(summary)} rows.\")\n",
        "    return summary.reset_index()\n",
        "\n",
        "# ===================================================\n",
        "#  CHARTING FUNCTION (NEW)\n",
        "# ===================================================\n",
        "def build_and_save_chart(df, symbol, timeframe, output_dir):\n",
        "    \"\"\"\n",
        "    Creates an interactive Plotly candlestick chart with Strat labels and saves as HTML.\n",
        "    \"\"\"\n",
        "    if df is None or df.empty or not isinstance(df.index, pd.DatetimeIndex):\n",
        "        print(f\"      Skipping chart for {symbol}-{timeframe}: Invalid data.\")\n",
        "        return\n",
        "\n",
        "    print(f\"      Generating chart for {symbol}-{timeframe}...\")\n",
        "    try:\n",
        "        fig = go.Figure()\n",
        "\n",
        "        # 1. Candlestick Trace\n",
        "        fig.add_trace(\n",
        "            go.Candlestick(\n",
        "                x=df.index,\n",
        "                open=df['open'],\n",
        "                high=df['high'],\n",
        "                low=df['low'],\n",
        "                close=df['close'],\n",
        "                name='OHLC'\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # 2. Strat Labels Trace\n",
        "        # Position labels slightly above high or below low\n",
        "        label_y_positions = np.where(\n",
        "            df['label'].isin(['2d']), df['low'] * 0.998, # Below low for 2d\n",
        "            df['high'] * 1.002 # Above high for 1, 2u, 3\n",
        "        )\n",
        "        # Adjust y-position further if hammer/shooter exists to avoid overlap\n",
        "        if 'is_hammer' in df.columns: label_y_positions = np.where(df['is_hammer'], label_y_positions * 1.002, label_y_positions)\n",
        "        if 'is_shooter' in df.columns: label_y_positions = np.where(df['is_shooter'], label_y_positions * 1.002, label_y_positions)\n",
        "\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=df.index[df['label'] != 'N/A'], # Don't plot N/A labels\n",
        "                y=label_y_positions[df['label'] != 'N/A'],\n",
        "                text=df['label'][df['label'] != 'N/A'],\n",
        "                mode='text',\n",
        "                name='Strat Labels',\n",
        "                textfont=dict(size=10),\n",
        "                showlegend=False\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # 3. Hammer/Shooter Indicators (Optional)\n",
        "        if 'is_hammer' in df.columns and df['is_hammer'].any():\n",
        "            hammer_df = df[df['is_hammer']]\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=hammer_df.index, y=hammer_df['high'] * 1.004, # Slightly higher position\n",
        "                mode='text', text='H', name='Hammer',\n",
        "                textfont=dict(color='blue', size=12), showlegend=False\n",
        "            ))\n",
        "        if 'is_shooter' in df.columns and df['is_shooter'].any():\n",
        "            shooter_df = df[df['is_shooter']]\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=shooter_df.index, y=shooter_df['high'] * 1.004, # Slightly higher position\n",
        "                mode='text', text='S', name='Shooter',\n",
        "                textfont=dict(color='red', size=12), showlegend=False\n",
        "            ))\n",
        "\n",
        "        # 4. Layout Configuration\n",
        "        fig.update_layout(\n",
        "            title=f\"{symbol} - {timeframe} - Strat Labels\",\n",
        "            xaxis_title=\"Date\",\n",
        "            yaxis_title=\"Price\",\n",
        "            xaxis_rangeslider_visible=False, # Disable range slider for cleaner look\n",
        "            hovermode=\"x unified\", # Show hover info for all traces at once\n",
        "            height=600 # Adjust height as needed\n",
        "        )\n",
        "\n",
        "        # 5. Save as HTML\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "        filepath = os.path.join(output_dir, f\"{symbol}_{timeframe}_Chart.html\")\n",
        "        fig.write_html(filepath)\n",
        "        # print(f\"      Chart saved to: {filepath}\") # Reduce noise\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"      ERROR generating chart for {symbol}-{timeframe}: {e}\")\n",
        "\n",
        "\n",
        "# ======================\n",
        "#  MAIN WORKFLOW\n",
        "# ======================\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function (Sequential Processing):\n",
        "    Loops through each symbol:\n",
        "    1. Fetches data.\n",
        "    2. Resamples.\n",
        "    3. Labels candles & adds formations.\n",
        "    4. Generates Charts (New).\n",
        "    5. Analyzes historical FTFC reversals.\n",
        "    6. Collects results.\n",
        "    After loop:\n",
        "    7. Combines detailed results.\n",
        "    8. Aggregates performance results.\n",
        "    9. Saves combined detailed (Excel & JSON) and summary (Excel & JSON).\n",
        "    \"\"\"\n",
        "    start_time = datetime.now()\n",
        "    print(f\"--- Starting Sequential Analysis at {start_time.strftime('%Y-%m-%d %H:%M:%S')} ---\")\n",
        "\n",
        "    all_historical_reversals_list = []\n",
        "    processed_symbols = []\n",
        "\n",
        "    # --- Loop through each symbol ---\n",
        "    for symbol in SYMBOLS:\n",
        "        print(f\"\\n--- Processing Symbol: {symbol} ---\")\n",
        "        symbol_data = {}\n",
        "        symbol_fetch_successful = True\n",
        "\n",
        "        # 1) Fetch Data\n",
        "        print(f\" Phase 1: Fetching Data for {symbol}\")\n",
        "        for function, intervals_or_name in TIMEFRAMES_API.items():\n",
        "            # (Fetching logic - same as previous version, omitted for brevity)\n",
        "            if isinstance(intervals_or_name, list): # Intraday\n",
        "                for tf_interval in intervals_or_name:\n",
        "                    time.sleep(0.5); df = get_data(symbol, function, interval=tf_interval)\n",
        "                    if df is not None and not df.empty: symbol_data[tf_interval] = df\n",
        "                    else: symbol_fetch_successful = False;\n",
        "            else: # Daily, Weekly, Monthly\n",
        "                tf_name = intervals_or_name\n",
        "                time.sleep(0.5); df = get_data(symbol, function)\n",
        "                if df is not None and not df.empty: symbol_data[tf_name] = df\n",
        "                else: symbol_fetch_successful = False;\n",
        "\n",
        "        if not symbol_data: print(f\" Warning: No data fetched for {symbol}. Skipping.\"); continue\n",
        "        required_base_tfs = [tf for tfs in TIMEFRAMES_API.values() for tf in (tfs if isinstance(tfs, list) else [tfs])]\n",
        "        if not all(tf in symbol_data for tf in required_base_tfs):\n",
        "             print(f\" Warning: Missing required base timeframes for {symbol}. Skipping analysis.\")\n",
        "             time.sleep(SLEEP_BETWEEN_SYMBOLS); continue\n",
        "\n",
        "        # 2) Resample\n",
        "        print(f\" Phase 2: Resampling Higher Timeframes for {symbol}\")\n",
        "        monthly_data = symbol_data.get('Monthly')\n",
        "        if monthly_data is not None and not monthly_data.empty and isinstance(monthly_data.index, pd.DatetimeIndex):\n",
        "            monthly_data_sorted = monthly_data.sort_index()\n",
        "            quarterly_data = resample_data(monthly_data_sorted.copy(), 'QE')\n",
        "            yearly_data = resample_data(monthly_data_sorted.copy(), 'YE')\n",
        "            if quarterly_data is not None and not quarterly_data.empty: symbol_data['Quarterly'] = quarterly_data\n",
        "            if yearly_data is not None and not yearly_data.empty: symbol_data['Yearly'] = yearly_data\n",
        "\n",
        "        # 3) Label Candles & Add Formations\n",
        "        print(f\" Phase 3: Labeling Candles for {symbol}\")\n",
        "        valid_data_exists_after_labeling = False\n",
        "        for timeframe, df_data in list(symbol_data.items()):\n",
        "            if df_data is not None and not df_data.empty and isinstance(df_data.index, pd.DatetimeIndex):\n",
        "                df_sorted = df_data.sort_index()\n",
        "                labeled_df = label_candlesticks(df_sorted)\n",
        "                if labeled_df is not None and 'label' in labeled_df.columns:\n",
        "                     final_df = add_hammer_shooter_columns(labeled_df)\n",
        "                     symbol_data[timeframe] = final_df # Update dict with processed data\n",
        "                     valid_data_exists_after_labeling = True\n",
        "                else: del symbol_data[timeframe] # Remove failed labeling\n",
        "            elif df_data is not None: del symbol_data[timeframe] # Remove non-datetime index data\n",
        "\n",
        "        if not valid_data_exists_after_labeling or not symbol_data:\n",
        "             print(f\" Warning: No valid labeled data remaining for {symbol}. Skipping analysis & charting.\")\n",
        "             time.sleep(SLEEP_BETWEEN_SYMBOLS); continue\n",
        "\n",
        "        # 4) Generate Charts (New Step)\n",
        "        if CREATE_CHARTS:\n",
        "            print(f\" Phase 4: Generating Charts for {symbol}\")\n",
        "            symbol_chart_dir = os.path.join(CHART_OUTPUT_DIR, symbol)\n",
        "            for timeframe, df_chart_data in symbol_data.items():\n",
        "                 build_and_save_chart(df_chart_data, symbol, timeframe, symbol_chart_dir)\n",
        "        else:\n",
        "             print(f\" Phase 4: Skipping Chart Generation for {symbol}\")\n",
        "\n",
        "\n",
        "        # 5) Analyze Historical FTFC Reversals\n",
        "        print(f\" Phase 5: Analyzing FTFC Reversals for {symbol}\")\n",
        "        symbol_reversals_df = analyze_historical_ftfc_reversals(\n",
        "            symbol_data, symbol, TIMEFRAME_ORDER,\n",
        "            lookahead=PERFORMANCE_LOOKAHEAD, min_higher_tfs=MIN_HIGHER_TFS_FOR_FTFC\n",
        "        )\n",
        "\n",
        "        # 6) Collect Results\n",
        "        if symbol_reversals_df is not None and not symbol_reversals_df.empty:\n",
        "            all_historical_reversals_list.append(symbol_reversals_df)\n",
        "            processed_symbols.append(symbol)\n",
        "            print(f\"   Finished analysis for {symbol}. Found {len(symbol_reversals_df)} reversals.\")\n",
        "        else:\n",
        "            print(f\"   No reversals found or analysis failed for {symbol}.\")\n",
        "\n",
        "        print(f\"   Pausing for {SLEEP_BETWEEN_SYMBOLS} sec...\")\n",
        "        time.sleep(SLEEP_BETWEEN_SYMBOLS)\n",
        "        # --- End of Symbol Loop ---\n",
        "\n",
        "\n",
        "    # --- Post-Loop Processing ---\n",
        "    print(f\"\\n--- Finished Processing All Symbols. Successfully processed: {processed_symbols} ---\")\n",
        "\n",
        "    # 7) Combine Detailed Results\n",
        "    if not all_historical_reversals_list:\n",
        "        print(\"No historical reversals found across any symbols.\")\n",
        "        combined_historical_reversals_df = pd.DataFrame()\n",
        "    else:\n",
        "        try:\n",
        "            print(\"Combining detailed results...\")\n",
        "            combined_historical_reversals_df = pd.concat(all_historical_reversals_list, ignore_index=True)\n",
        "            print(f\"Combined results shape: {combined_historical_reversals_df.shape}\")\n",
        "            # Ensure 'Reversal Time' is datetime after concat\n",
        "            if 'Reversal Time' in combined_historical_reversals_df.columns:\n",
        "                 combined_historical_reversals_df['Reversal Time'] = pd.to_datetime(combined_historical_reversals_df['Reversal Time'])\n",
        "        except Exception as e:\n",
        "            print(f\"Error combining historical reversal results: {e}\")\n",
        "            combined_historical_reversals_df = pd.DataFrame()\n",
        "\n",
        "    # 8) Aggregate Combined Performance Results\n",
        "    print(\"\\n--- Aggregating Combined Performance Summary ---\")\n",
        "    performance_summary_df = aggregate_performance_results(combined_historical_reversals_df)\n",
        "\n",
        "\n",
        "    # --- 9) Save Combined Results (Excel & JSON) ---\n",
        "    today_str = date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    # Save Detailed Results\n",
        "    print(\"\\n--- Saving Combined Detailed Performance Results ---\")\n",
        "    if not combined_historical_reversals_df.empty:\n",
        "        detailed_excel_filename = f\"{today_str}_FTFC_Reversal_Performance_Detailed_ALL.xlsx\"\n",
        "        detailed_json_filename = f\"{today_str}_FTFC_Reversal_Performance_Detailed_ALL.json\"\n",
        "        try:\n",
        "            df_to_save_detailed = combined_historical_reversals_df.copy()\n",
        "            # Convert datetime to string for JSON compatibility (ISO format)\n",
        "            if 'Reversal Time' in df_to_save_detailed.columns:\n",
        "                 df_to_save_detailed['Reversal Time_str'] = df_to_save_detailed['Reversal Time'].dt.strftime('%Y-%m-%dT%H:%M:%S') # Create string version for JSON\n",
        "                 # Make timezone naive for Excel\n",
        "                 if pd.api.types.is_datetime64tz_dtype(df_to_save_detailed['Reversal Time']):\n",
        "                      df_to_save_detailed['Reversal Time'] = df_to_save_detailed['Reversal Time'].dt.tz_localize(None)\n",
        "\n",
        "            # Save Excel\n",
        "            df_to_save_detailed.drop(columns=['Reversal Time_str'], errors='ignore').to_excel(detailed_excel_filename, index=False, engine='openpyxl')\n",
        "            print(f\"   Combined detailed results saved to: {detailed_excel_filename}\")\n",
        "            # Save JSON\n",
        "            df_to_save_detailed.drop(columns=['Reversal Time'], errors='ignore').rename(columns={'Reversal Time_str':'Reversal Time'}).to_json(detailed_json_filename, orient='records', indent=4)\n",
        "            print(f\"   Combined detailed results saved to: {detailed_json_filename}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   Error saving combined detailed performance results: {e}\")\n",
        "    else:\n",
        "        print(\"   No combined historical reversals found, skipping detailed results save.\")\n",
        "\n",
        "    # Save Summary Results\n",
        "    print(\"\\n--- Saving Combined Performance Summary ---\")\n",
        "    if performance_summary_df is not None and not performance_summary_df.empty:\n",
        "        summary_excel_filename = f\"{today_str}_FTFC_Reversal_Performance_Summary_ALL.xlsx\"\n",
        "        summary_json_filename = f\"{today_str}_FTFC_Reversal_Performance_Summary_ALL.json\"\n",
        "        try:\n",
        "            # Save Excel\n",
        "            performance_summary_df.to_excel(summary_excel_filename, index=False, engine='openpyxl')\n",
        "            print(f\"   Combined aggregated performance summary saved to: {summary_excel_filename}\")\n",
        "            # Save JSON\n",
        "            performance_summary_df.to_json(summary_json_filename, orient='records', indent=4)\n",
        "            print(f\"   Combined aggregated performance summary saved to: {summary_json_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   Error saving combined performance summary: {e}\")\n",
        "    else:\n",
        "        print(\"   No performance summary generated, skipping summary results save.\")\n",
        "\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    print(f\"\\n--- Analysis Complete at {end_time.strftime('%Y-%m-%d %H:%M:%S')} ---\")\n",
        "    print(f\"Total execution time: {end_time - start_time}\")\n",
        "\n",
        "\n",
        "# ======================\n",
        "#  SCRIPT EXECUTION\n",
        "# ======================\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}